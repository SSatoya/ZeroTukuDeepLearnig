{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9b61c9",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf065434",
   "metadata": {},
   "source": [
    "- ch04で行った、重みパラメータの勾配は数値微分によって求めた。\n",
    "    - but...数値微分法では計算に時間がかかる\n",
    "    - --> 効率的に行うために **「誤差伝播法」**\n",
    "- 誤差伝播法を理解するために...\n",
    "    - 数式\n",
    "    - **計算グラフ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0588be7",
   "metadata": {},
   "source": [
    "# 計算グラフ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71aee3b",
   "metadata": {},
   "source": [
    "- **計算グラフ**とは？\n",
    "    - 計算の過程をグラフよってあらわしたもの\n",
    "    - 複数ノードとエッジによって表現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13dc1c",
   "metadata": {},
   "source": [
    "## 計算グラフで解く\n",
    "- `p124~`参照\n",
    "- 計算グラフを使用して問題を解くには...?\n",
    "    1. 計算グラフを構築\n",
    "    2. 計算グラフ上で計算を左から右に進める(**順伝播**)\n",
    "- 順伝播について\n",
    "    - 計算グラフの出発点から終点への伝播\n",
    "    - 逆方向は...?\n",
    "        - **逆伝播**という\n",
    "        - この動きは、微分を計算する上で重要\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c2eaa",
   "metadata": {},
   "source": [
    "## 局所的計算\n",
    "- 自分に関係している部分だけから、次の結果を出力する\n",
    "- 全体が複雑であっても、局所的な計算なら単純であり、その結果を伝達することで複雑な計算の結果が得られる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4167e6f0",
   "metadata": {},
   "source": [
    "## なぜ計算グラフを使うの？\n",
    "- 計算グラフの利点：\n",
    "    - 順伝播と逆伝播によって、各変数の微分の値を効率的に求めることができる\n",
    "    - Ex)リンゴが1円値上がりしたら、最終的な支出金額は何円増加するか"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e634e",
   "metadata": {},
   "source": [
    "# 連鎖律"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a569a1",
   "metadata": {},
   "source": [
    "- 逆伝播\n",
    "    - 右から左に伝達していく\n",
    "    - 局所的な微分を伝達する原理は**連鎖律**によるもの"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a67d",
   "metadata": {},
   "source": [
    "## 計算グラフの逆伝播\n",
    "- `p129~`参照\n",
    "- Ex)\n",
    "    - $y = f(x)$ とする\n",
    "    - $$x → f → y$$\n",
    "    - $$E\\frac{∂y}{∂x} ← f ← E$$\n",
    "    - 例えば、$y = f(x) = x^2$ のとき、\n",
    "        - $$\\frac{∂y}{∂x} = 2x$$\n",
    "        - この局所的な微分を上流から伝達された値（E）に乗算して、前のノードに渡す\n",
    "- ポイント：\n",
    "    - 目的とする微分の値が簡単に求めることができる\n",
    "    - これは、連鎖律の原理から説明可能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae236e",
   "metadata": {},
   "source": [
    "## 連鎖律とは？\n",
    "- まずは合成関数から\n",
    "    - 合成関数＝複数の関数によって構成される関数\n",
    "    - Ex)  $z = (x + y)^2$ とき、zは２つの式で構成されているといえる\n",
    "    $$z = t^2$$\n",
    "    $$t = x + y$$\n",
    "\n",
    "- **連鎖律の原理**\n",
    "    - ある関数が合成関数であらわされる場合、その合成関数の微分は合成関数を構成するそれぞれの関数の微分の積によってあらわすことが可能\n",
    "    - つまり...?\n",
    "        - $z = (x + y)^2$ であれば、\n",
    "        $$\\frac{∂z}{∂x}は、\\frac{∂z}{∂t}と\\frac{∂t}{∂x}の積によってあらわせる$$\n",
    "        $$\\frac{∂z}{∂x} = \\frac{∂z}{∂t}\\frac{∂t}{∂x}$$\n",
    "- 連鎖率を使って、$\\frac{∂z}{∂x}$ を求めてみる\n",
    "$$\\frac{∂z}{∂t} = 2t$$\n",
    "$$\\frac{∂t}{∂x} = 1$$\n",
    "- よって\n",
    "$$\\frac{∂z}{∂x} = \\frac{∂z}{∂t}\\frac{∂t}{∂x} = 2t・1 = 2(x + y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa889d",
   "metadata": {},
   "source": [
    "## 連鎖率と計算グラフ\n",
    "- p131～参照"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb798f",
   "metadata": {},
   "source": [
    "# 逆伝播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f9dcb",
   "metadata": {},
   "source": [
    "## 加算ノードの逆伝播\n",
    "- 加算の場合は、上流から伝わった微分に**1を乗算**して下流に流す\n",
    "    - つまり、入力値をそのまま次のノードへ流すだけ\n",
    "## 乗算ノードの逆伝播\n",
    "- 乗算の場合は、上流から伝わった微分に、順伝播の際の入力値を **\"ひっくり返した値\"を乗算**して下流へ流す\n",
    "    - ひっくり返した値って？\n",
    "        - 「×ノード」にxとyを入力した場合：\n",
    "            - 順伝播で`x`の信号であれば逆伝播では`y`\n",
    "            - 順伝播で`y`の信号であれば逆伝播では`x`\n",
    "            - と、ひっくり返した関係になっている\n",
    "        - Ex) $10 × 5 = 50$ の場合\n",
    "            - 上流から`1.3`の値が流れてくる\n",
    "            - 10の信号側：1.3 × 5 = 6.5\n",
    "            - 5の信号側：1.3 × 10 = 13\n",
    "            - が、それぞれ下流に流れていく\n",
    "- 逆伝播の値の見方は？\n",
    "    - 伝播の値をそれぞれｘ、yとする\n",
    "    - 2つの対象が同じ量だけ増加したら、１つの対象はxの大きさで最終的な結果に影響を与え、もう一方の対象はyの大きさで影響を与えると解釈可能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be48198",
   "metadata": {},
   "source": [
    "# 単純なレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704dcbe3",
   "metadata": {},
   "source": [
    "- 計算グラフのノードについて\n",
    "    - 乗算ノード：乗算レイヤ（MulLayer）\n",
    "    - 加算ノード：加算レイヤ（AddLayer）\n",
    "- ニューラルネットワークを構築する「層（レイヤ）」を一つのクラスで実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb10376",
   "metadata": {},
   "source": [
    "## 乗算レイヤの実装\n",
    "- レイヤは`forward()`と`backward()`という共通のメソッドを持つように実装\n",
    "    - `forward()`：順伝播\n",
    "    - `backward()`：逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95ff89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        インスタンス変数xとyを初期化\n",
    "        xとyは順伝播の際に入力値を保持するために使用\n",
    "        \"\"\"\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x, yを受け取り、xとyの積を計算する\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        上流から伝わってきた微分（dout）に対して、順伝播の\"ひっくり返した値\"を乗算する\n",
    "        \"\"\"\n",
    "        dx = dout * self.y  # xとyをひっくり返す\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01de14df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward：\n",
      "price=220.00000000000003\n",
      "\n",
      "backward：\n",
      "dapple_price= 1.1, \n",
      "dtax= 200, \n",
      "dapple= 2.2, \n",
      "dapple_num= 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "apple = 100  # りんごの価格\n",
    "apple_num = 2  # りんごの個数\n",
    "tax = 1.1  # 税率\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()  # りんごの価格と個数を掛けるレイヤー\n",
    "mul_tax_layer = MulLayer()  # 税率を掛けるレイヤー\n",
    "\n",
    "# forward(順伝播)\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # りんごの合計価格\n",
    "price = mul_tax_layer.forward(apple_price, tax)  # 税込み価格\n",
    "\n",
    "print(f\"forward：\\nprice={price}\\n\")\n",
    "\n",
    "# backward(逆伝播)\n",
    "dprice = 1  # 出力の微分（価格に対する微分）\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)  # 税率に対する微分\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # りんごの価格と個数に対する微分\n",
    "print(f\"backward：\\ndapple_price= {dapple_price}, \\ndtax= {dtax}, \\ndapple= {dapple}, \\ndapple_num= {dapple_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c12d12",
   "metadata": {},
   "source": [
    "## 加算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec410337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初期化（何も行わない）\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x, yを受け取り、xとyの和を計算する\n",
    "        \"\"\"\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        上流から伝わってきた微分（dout）をそのまま返す\n",
    "        \"\"\"\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d2f45a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price：715.0000000000001\n",
      "\n",
      "Backward:\n",
      "dall_price= 1.1, \n",
      "dtax= 650, \n",
      "dapple_price= 1.1, \n",
      "dorange_price= 1.1, \n",
      "dapple= 2.2, \n",
      "dapple_num= 110.00000000000001, \n",
      "dorange= 3.3000000000000003, \n",
      "dorange_num= 165.0\n"
     ]
    }
   ],
   "source": [
    "apple = 100 # りんごの価格\n",
    "apple_num = 2 # りんごの個数\n",
    "\n",
    "orange = 150 # オレンジの価格\n",
    "orange_num = 3 # オレンジの個数\n",
    "\n",
    "tax = 1.1 # 税率\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()  # りんごの価格と個数を掛けるレイヤー\n",
    "mul_orange_layer = MulLayer()  # オレンジの価格と個数を掛けるレイヤー\n",
    "add_apple_orange_layer = AddLayer()  # りんごとオレンジの合計価格を計算するレイヤー\n",
    "mul_tax_layer = MulLayer()  # 税率を掛けるレイヤー\n",
    "\n",
    "# forward(順伝播)\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # りんごの合計価格\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # オレンジの合計価格\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # りんごとオレンジの合計価格\n",
    "price = mul_tax_layer.forward(all_price, tax)  # 税込み価格\n",
    "print(f\"Price：{price}\\n\")\n",
    "\n",
    "# backward(逆伝播)\n",
    "dprice = 1  # 出力の微分（価格に対する微分）\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # 税率に対する微分\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # りんごとオレンジの合計価格に対する微分\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # りんごの価格と個数に対する微分\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # オレンジの価格と個数に対する微分\n",
    "print(f\"Backward:\\ndall_price= {dall_price}, \\ndtax= {dtax}, \\ndapple_price= {dapple_price}, \\ndorange_price= {dorange_price}, \\ndapple= {dapple}, \\ndapple_num= {dapple_num}, \\ndorange= {dorange}, \\ndorange_num= {dorange_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e857cd4c",
   "metadata": {},
   "source": [
    "# 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ae4b9",
   "metadata": {},
   "source": [
    "- 計算グラフの考え方をニューラルネットワークに適用する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd80ad9",
   "metadata": {},
   "source": [
    "## ReLUレイヤ\n",
    "- 活性化関数として使用されるReLU\n",
    "    - 入力xが0より大きければ、そのままその値を返す\n",
    "    - 入力yが0以下であれば、0を返す\n",
    "```math\n",
    "f(x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "x & (x > 0) \\\\\n",
    "0 & (x ≦ 0)\n",
    "\\end{array}\n",
    "\\right.\n",
    "```\n",
    "- xに関するyの微分の式は以下のようになる\n",
    "```math\n",
    "\\frac{∂y}{∂x} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & (x > 0) \\\\\n",
    "0 & (x ≦ 0)\n",
    "\\end{array}\n",
    "\\right.\n",
    "```\n",
    "- 上記の式から...\n",
    "    - 順伝播時の入力である**xが0より大きい**時：\n",
    "        - 逆伝播は上流の値をそのまま下流に流す\n",
    "    - 順伝播時の入力である**xが0以下**の時：\n",
    "        - 逆伝播では、下流への信号はストップする（0が流れて終わり）\n",
    "\n",
    "- 実装\n",
    "    - 入力はNumPyの配列が入力される\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "572e37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        True/Falseのマスクを保持するためのインスタンス変数maskを初期化\n",
    "        \"\"\"\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        xを受け取り、ReLU関数を適用する\n",
    "        x：入力値（NumPy配列）\n",
    "        出力：ReLU関数を適用した結果（NumPy配列）\n",
    "        \"\"\"\n",
    "        self.mask = (x <= 0)  # xが0以下の要素をTrueにするマスクを作成\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0  # マスクされた要素を0にする\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        上流から伝わってきた微分（dout）に対して、ReLUのマスクを適用する\n",
    "        dout：上流からの微分（NumPy配列）\n",
    "        出力：ReLU関数の微分結果（NumPy配列）\n",
    "        \"\"\"\n",
    "        dout[self.mask] = 0  # マスクされた要素に対しては微分を0にする\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f7f231",
   "metadata": {},
   "source": [
    "## Sigmoidレイヤ\n",
    "- シグモイド関数の関数\n",
    "$$y=\\frac{1}{1+exp(-x)}$$\n",
    "- 計算グラフでは、順伝播の場合は上流から順に以下のようになる\n",
    "    ```math\n",
    "    \\begin{aligned}\n",
    "    &\\text{「×ノード」} \\quad x \\times -1 = -x \\\\\n",
    "    &\\text{「expノード」} \\quad \\exp(-x) \\\\\n",
    "    &\\text{「+ノード」} \\quad \\exp(-x) + 1 \\\\\n",
    "    &\\text{「/ノード」} \\quad \\frac{1}{1+\\exp(-x)} = y\n",
    "    \\end{aligned}\n",
    "    ```\n",
    "\n",
    "- 逆伝播の流れ\n",
    "    1. 「/ノード」$y = \\frac{1}{x}$ の微分\n",
    "        $$\\frac{∂y}{∂x} = -\\frac{1}{x^2} = -y^2$$\n",
    "        - 上流の値に対して $-y^2$ を乗算して下流へ伝播\n",
    "    2. 「+ノード」\n",
    "        - 上流の値をそのまま下流へ流す\n",
    "    3. 「expノード」$y = exp(x)$ の微分\n",
    "        $$\\frac{∂y}{∂x} = exp(x)$$\n",
    "        - 上流の値に対して`exp(x)`を乗算して下流へ伝播\n",
    "    4. 「×ノード」\n",
    "        - \"ひっくり返した値\"を乗算\n",
    "        - x側を求めるには、上流の値に対して`-1`を乗算して下流へ伝播\n",
    "- 上記の逆伝播の流れより、上流の微分した値が </br> \n",
    "    $$\\frac{∂L}{∂y}$$  \n",
    "    であった場合、ステップ1~4を経ると、\n",
    "    1. $$-\\frac{∂L}{∂y}y^2$$\n",
    "    2. $$-\\frac{∂L}{∂y}y^2$$\n",
    "    3. $$-\\frac{∂L}{∂y}y^2exp(x)$$\n",
    "    4. $$\\frac{∂L}{∂y}y^2exp(x)$$\n",
    "    となる。\n",
    "- ここで、上記の式を整理すると...\n",
    "```math\n",
    "\\frac{∂L}{∂y}y^2exp(x) = \\frac{∂L}{∂y}\\frac{1}{(1+exp(-x))^2}exp(x) \\\\\n",
    "\\hspace{75pt}= \\frac{∂L}{∂y}\\frac{1}{1+exp(-x)}\\frac{exp(x)}{1+exp(x)} \\\\\n",
    "= \\frac{∂L}{∂y}y(1-y)\n",
    "```\n",
    "- つまり、Sigmoidレイヤに上流の値を通すと中身はあまり考えなくとも、上記の式の出力だけに集中することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "269b5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        シグモイド関数の出力を保持するためのインスタンス変数outを初期化\n",
    "        \"\"\"\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        シグモイド関数を適用する\n",
    "        x：入力値（NumPy配列）\n",
    "        出力：シグモイド関数を適用した結果（NumPy配列）\n",
    "        \"\"\"\n",
    "        out = 1 / (1 + np.exp(-x))  # シグモイド関数の計算\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        上流から伝わってきた微分（dout）に対して、シグモイド関数の微分を適用する\n",
    "        \"\"\"\n",
    "        dx = dout * self.out * (1 - self.out)  # シグモイド関数の微分\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49dfc3",
   "metadata": {},
   "source": [
    "# Affine/Softmaxレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9a4d1",
   "metadata": {},
   "source": [
    "## Affineレイヤ\n",
    "- ニューラルネットワークの順伝播\n",
    "    - 行列の内積を用いた\n",
    "    - ニューロンの重み付き和は、`Y = np.dot(X, W) + B`と計算できる\n",
    "        - この時、対応する次元数は一致させる必要がある\n",
    "        - (2,)<->(2,3)<->(3,)\n",
    "    - そしてこの`Y`が活性化関数によって変換され、次の層へと伝播する\n",
    "    - このようなニューラルネットワークの順伝播で行う行列の内積は、「**アフィン変換**」と呼ばれる\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ad0668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16487319 0.58380276]\n",
      "[[0.84589124 0.50932981 0.28588086]\n",
      " [0.89121406 0.75717278 0.61157984]]\n",
      "[0.64316609 0.35932206 0.37141972]\n",
      "Xの形状: (2,)\n",
      "Wの形状: (2, 3)\n",
      "bの形状: (3,)\n",
      "Y: [1.30292411 0.88533645 0.77559581]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(2)  # 要素数2つ（2列）入力データ\n",
    "W = np.random.rand(2, 3)  # 2x3の重み行列\n",
    "b = np.random.rand(3)  # 3次元のバイアス\n",
    "print(X)\n",
    "print(W)\n",
    "print(b)\n",
    "print(\"Xの形状:\", X.shape)\n",
    "print(\"Wの形状:\", W.shape)\n",
    "print(\"bの形状:\", b.shape)\n",
    "\n",
    "# 内積を求める\n",
    "Y = np.dot(X, W) + b  # XとWの内積にバイアスbを加える\n",
    "print(\"Y:\", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6c700",
   "metadata": {},
   "source": [
    "- ここで、順伝播である行列の内積とバイアスの和を計算グラフであらわす\n",
    "    - 内積は「dotノード」として表現\n",
    "    - ()内は、それぞれの次元をあらわす\n",
    "    - X, W, Bはそれぞれ入力値、重み、バイアスをあらわし、すべて**行列**である\n",
    "    ```math\n",
    "    \\begin{aligned}\n",
    "    &\\text{「dotノード」} \\quad dot(X(2,), W(2, 3)) = X・W(3,)\\\\\n",
    "    &\\text{「+ノード」} \\quad X・W(3,) + B(3,) = Y(3,) \\\\\n",
    "    \\end{aligned}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfabea6",
   "metadata": {},
   "source": [
    "- 次に、逆伝播について考える(p149~参照)\n",
    "    - 行列の逆伝播を求める時、要素ごとに考えると良い\n",
    "    - 次の式が得られる\n",
    "        ```math\n",
    "        \\frac{∂L}{∂X} = \\frac{∂L}{∂Y}・W^T\\\\\n",
    "        \\frac{∂L}{∂W} = X^T・\\frac{∂L}{∂Y}\n",
    "        ```\n",
    "    - Tは転置を表現\n",
    "        - 転置：Wの(i, j)の要素を(j, i)の要素に入れ替えること\n",
    "    1. 「+ノード」\n",
    "        - そのまま流す\n",
    "    2. 「dotノード」\n",
    "        ```math\n",
    "        \\frac{∂L}{∂X} = \\frac{∂L}{∂Y}・W^T\\\\\n",
    "        \\frac{∂L}{∂W} = X^T・\\frac{∂L}{∂Y}\n",
    "        ```\n",
    "    \n",
    "- この辺ムズイ(ーー;)p149~の図をしっかり参照すること"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd283402",
   "metadata": {},
   "source": [
    "## バッチ版Affineレイヤ\n",
    "- N個のデータをまとめて順伝播する\n",
    "    - データのまとまり＝「バッチ」\n",
    "- １つのデータの時との違い：\n",
    "    - 入力データが(N, 2)に変更\n",
    "    - それ以外は、すべて同じ！\n",
    "- 注意点：**バイアスの加算**\n",
    "    - バイアスの加算は、それぞれのデータに加算される\n",
    "    - これにより、逆伝播の際は、それぞれのデータの逆伝播の値がバイアスの要素に集約される必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f1993c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [11. 12. 13.]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.0, 0.0, 0.0],[10, 10, 10]]) \n",
    "b = np.array([1.0, 2.0, 3.0])\n",
    "print(x + b)  # ブロードキャストにより、bがxの各データに加算される\n",
    "\n",
    "dY = np.array([[1, 2, 3],[4, 5, 6]])  # 出力の微分（dY）を初期化\n",
    "db = np.sum(dY, axis=0)  # バイアスの微分（db）を計算\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0df14e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        \"\"\"\n",
    "        重みW、バイアスb、入力xを初期化\n",
    "        \"\"\"\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None  # 重みの微分\n",
    "        self.db = None  # バイアスの微分\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        xを受け取り、重みWとバイアスbを用いて内積を計算する\n",
    "        x：入力値（NumPy配列）\n",
    "        出力：線形変換の結果（NumPy配列）\n",
    "        \"\"\"\n",
    "        self.x = x  # 入力値を保持\n",
    "        out = np.dot(x, self.W) + self.b  # 内積とバイアスの加算\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        上流から伝わってきた微分（dout）に対して、重みWとバイアスbの微分を計算する\n",
    "        dout：上流からの微分（NumPy配列）\n",
    "        出力：入力xの微分（NumPy配列）\n",
    "        \"\"\"\n",
    "        dx = np.dot(dout, self.W.T)  # 入力の微分\n",
    "        self.dW = np.dot(self.x.T, dout)  # 重みの微分\n",
    "        self.db = np.sum(dout, axis=0)  # バイアスの微分\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5b5d7",
   "metadata": {},
   "source": [
    "## Softmax-with-Lossレイヤ\n",
    "- 最後に出力層であるソフトマックス関数について\n",
    "    - 入力された値を正規化して出力\n",
    "    - 合計が１になるように変形して出力\n",
    "- 「Softmax関数」＋「損失関数である交差エントロピー誤差」＝「Softmax-with-Loss」\n",
    "    - ソフトマックス関数（Softmaxレイヤ）は、入力値(a_n)を正規化した値(y_n)を出力\n",
    "    - 交差エントロピー誤差（Cross Entropy Errorレイヤ）は、ソフトマックスの出力(y_n)と、教師ラベル(t_n)を受け取りそれらのデータから損失Lを出力する\n",
    "- 逆伝播について\n",
    "    - Softmaxレイヤから逆伝播は、`(y_1 - t_1, y_2 - t_2, y_3 -t_3)`という結果になる\n",
    "        - かなり\"きれい\"な状態になる！！\n",
    "            - これは、交差エントロピー誤差という関数の性質\n",
    "        - これは、Softmaxレイヤの出力と教師ラベルの差分である\n",
    "        - この差分である誤差が前レイヤへ伝わっていく\n",
    "            - **重要な性質！！**\n",
    "- ニューラルネットワークの学習の目的：\n",
    "    - 出力を今日調べるに近づけるように、重みパラメータを調整すること\n",
    "    - ニューラルネットワークの出力と教師ラベルとの誤差を効率的に前レイヤに伝える必要がある\n",
    "    - `(y_1 - t_1, y_2 - t_2, y_3 -t_3)`という結果は、ソフトマックス関数の出力と教師ラベルの差！！\n",
    "        - ニューラルネットワークの出力と教師ラベルの差を表現\n",
    "    - Ex)\n",
    "        - 認識誤差が大きい場合\n",
    "            - 教師ラベル：（0, 1, 0）\n",
    "            - Softmaxレイヤの出力：(0.3, 0.2, 0.5)\n",
    "            - 正解ラベルに対する確率は20%であり、正しい認識ができていない\n",
    "            - この時の逆伝播は、(0.3-0, 0.2-1.0, 0.5-0)=(0.3, -0.8, 0.5)と大きな誤差を伝播可能\n",
    "        - 認識誤差が小さい場合\n",
    "            - 教師ラベル：(0, 1, 0)\n",
    "            - Softmaxxレイヤの出力：(0.01, 0.99, 0)\n",
    "            - 正解ラベルに対する確率は99%であり、ほぼ正しい認識ができている\n",
    "            - この時の逆伝播は、(0.01-0, 0.99-1, 0-0)=(0.01, -0.01. 0)と非常に小さな誤差になる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "985f6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリをパスに追加\n",
    "from deep_learning_from_scratch.common.functions import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        ソフトマックス関数の出力と正解ラベルを保持するためのインスタンス変数\n",
    "        \"\"\"\n",
    "        self.loss = None  # 損失値\n",
    "        self.y = None  # ソフトマックス関数の出力\n",
    "        self.t = None  # 正解ラベル（One-hotエンコーディングされた形式）\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        ソフトマックス関数を計算し、クロスエントロピー損失を求める\n",
    "        x：入力値（NumPy配列）\n",
    "        t：正解ラベル（NumPy配列）\n",
    "        出力：損失値\n",
    "        \"\"\"\n",
    "        self.t = t  # 正解ラベルを保持\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        \"\"\"\n",
    "        損失の微分を計算する\n",
    "        dout：上流からの微分（デフォルトは1）\n",
    "        出力：ソフトマックス関数の出力に対する微分\n",
    "        \"\"\"\n",
    "        batch_size = self.t.shape[0]  # バッチサイズを取得\n",
    "        dx = (self.y - self.t) / batch_size  # データ１個あたりの誤差が前レイヤーに伝播する\n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc598b04",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de225ff",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの学習の全体図\n",
    "- 前提  \n",
    "    - 「学習」：\n",
    "        - 重みとバイアスのパラメータを訓練データに適応するように調整すること\n",
    "- ステップ1（ミニバッチ）\n",
    "    - 訓練データの中から無作為に一部のデータを選び出す\n",
    "- ステップ2（勾配の算出）\n",
    "    - 各重みパラメータに関する損失関数の勾配を求める\n",
    "- ステップ3（パラメータの更新）\n",
    "    - 重みパラメータを勾配方向に微小量だけ更新する\n",
    "- ステップ4（繰り返す）\n",
    "    - ステップ1~3を繰り返す    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cacb12",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法に対応したニューラルネットワークの実装\n",
    "- TwoLayerNet\n",
    "    - 2層のニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリをパスに追加\n",
    "from deep_learning_from_scratch.common.functions import *\n",
    "from deep_learning_from_scratch.common.gradient import numerical_gradient  # 勾配を計算する関数\n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict  # 順序付き辞書を使用, 追加した要素の順序を覚えることが可能\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \"\"\"\n",
    "        二層のニューラルネットワークを初期化する\n",
    "        input_size：入力層のサイズ\n",
    "        hidden_size：隠れ層のサイズ\n",
    "        output_size：出力層のサイズ\n",
    "        weight_init_std：重みの初期化標準偏差（デフォルトは0.01）\n",
    "        \"\"\"\n",
    "        # 重みとバイアスを初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤーを初期化\n",
    "        self.layers = OrderedDict()  # 順序付き辞書を使用してレイヤーを保持\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])  # Affineレイヤーを追加\n",
    "        self.layers['Relu1'] = Relu()  # ReLUレイヤーを追加\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])  # 2つ目のAffineレイヤーを追加\n",
    "        \n",
    "        # 損失関数を初期化\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        入力xに対して予測を行う\n",
    "        x：入力データ（NumPy配列）\n",
    "        出力：予測結果（NumPy配列）\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        入力xと正解ラベルtに対して損失を計算する\n",
    "        x：入力データ（NumPy配列）\n",
    "        t：正解ラベル（NumPy配列）\n",
    "        出力：損失値\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        入力xと正解ラベルtに対して精度を計算する\n",
    "        x：入力データ（NumPy配列）\n",
    "        t：正解ラベル（NumPy配列）\n",
    "        出力：精度（0から1の範囲）\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)  # tがOne-hotエンコーディングされている場合、インデックスに変換\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        入力xと正解ラベルtに対して、パラメータの数値微分を計算する\n",
    "        x：入力データ（NumPy配列）\n",
    "        t：正解ラベル（NumPy配列）\n",
    "        出力：パラメータの勾配（辞書形式）\n",
    "        \"\"\"\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])  # W1の勾配を計算\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])  # b1の勾配を計算\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])  # W2の勾配を計算\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])  # b2の勾配を計算\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        入力xと正解ラベルtに対して、パラメータの勾配を計算する\n",
    "        x：入力データ（NumPy配列）\n",
    "        t：正解ラベル（NumPy配列）\n",
    "        出力：パラメータの勾配（辞書形式）\n",
    "        \"\"\"\n",
    "        # 順伝播(forward)\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 逆伝播(backward)\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f72445",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法の勾配確認\n",
    "- 勾配を求める方法は２つ\n",
    "    1. 数値微分によって求める\n",
    "        - 単純\n",
    "        - 非効率的\n",
    "    2. 解析的に数式を解いて求める\n",
    "        - 複雑\n",
    "        - 誤差逆伝播法を用いることで、大量のパラメータが存在しても効率的に計算可能\n",
    "- 数値微分の必要性\n",
    "    - 数値微分は、非効率的なら誤差逆伝播法だけでいいのでは...?\n",
    "    - **誤差逆伝播法の正しさを確認するための使用**できる！！\n",
    "        - 数値微分は、単純な構造なのでミスしにくい\n",
    "    - この誤差逆伝播法の勾配結果と数値微分の勾配結果がほぼ一致することを確認する作業を**勾配確認**という\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e116d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1の絶対誤差の平均: 5.730597504455014e-10\n",
      "b1の絶対誤差の平均: 3.6196919858059572e-09\n",
      "W2の絶対誤差の平均: 6.917187013379909e-09\n",
      "b2の絶対誤差の平均: 1.4030637523892996e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir) \n",
    "from deep_learning_from_scratch.dataset.mnist import load_mnist  # MNISTデータセットを読み込む関数\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# MNISTデータセットを読み込む\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)  # ネットワークを初期化\n",
    "\n",
    "x_batch = x_train[:3]  # バッチサイズ3の入力データを取得(3, 784)\n",
    "t_batch = t_train[:3]  # バッチサイズ3の正解ラベルを取得 (3,)\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)  # 数値微分を計算\n",
    "grad_backprop = network.gradient(x_batch, t_batch)  # 逆伝播を使用して勾配を計算\n",
    "\n",
    "\n",
    "# 各重みの絶対誤差の平均を求める\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_numerical[key] - grad_backprop[key]))\n",
    "    print(f\"{key}の絶対誤差の平均: {diff}\")  # 絶対誤差の平均を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e2dd5",
   "metadata": {},
   "source": [
    "- 上記の結果から、数値微分と誤差逆伝播法でそれぞれ求めた勾配の差葉かなり小さいことがわかる！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739244a",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法を使った学習\n",
    "- 誤差逆伝播法を使ったニューラルネットワークの学習の実装を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9ceba957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション 0, 損失: 2.301524659657293, 学習精度: 0.09683333333333333, テスト精度: 0.0979\n",
      "イテレーション 600, 損失: 0.39448905606157036, 学習精度: 0.9010166666666667, テスト精度: 0.9038\n",
      "イテレーション 1200, 損失: 0.24767847017670683, 学習精度: 0.9231333333333334, テスト精度: 0.9267\n",
      "イテレーション 1800, 損失: 0.17545380701935656, 学習精度: 0.9367333333333333, テスト精度: 0.9352\n",
      "イテレーション 2400, 損失: 0.12473206122853889, 学習精度: 0.9448333333333333, テスト精度: 0.9434\n",
      "イテレーション 3000, 損失: 0.07413857703530645, 学習精度: 0.9534666666666667, テスト精度: 0.9513\n",
      "イテレーション 3600, 損失: 0.12077940488068836, 学習精度: 0.9575666666666667, テスト精度: 0.9548\n",
      "イテレーション 4200, 損失: 0.06904663794774413, 学習精度: 0.9626166666666667, テスト精度: 0.9606\n",
      "イテレーション 4800, 損失: 0.0613175648152332, 学習精度: 0.9648833333333333, テスト精度: 0.9612\n",
      "イテレーション 5400, 損失: 0.0923907582159141, 学習精度: 0.96815, テスト精度: 0.9628\n",
      "イテレーション 6000, 損失: 0.10574318820933709, 学習精度: 0.97125, テスト精度: 0.9655\n",
      "イテレーション 6600, 損失: 0.10109669456913004, 学習精度: 0.9734166666666667, テスト精度: 0.9669\n",
      "イテレーション 7200, 損失: 0.11210692564866402, 学習精度: 0.976, テスト精度: 0.9685\n",
      "イテレーション 7800, 損失: 0.0383735928105277, 学習精度: 0.9763166666666667, テスト精度: 0.9673\n",
      "イテレーション 8400, 損失: 0.0490458786560431, 学習精度: 0.9774333333333334, テスト精度: 0.9691\n",
      "イテレーション 9000, 損失: 0.10329584492483175, 学習精度: 0.9790833333333333, テスト精度: 0.9685\n",
      "イテレーション 9600, 損失: 0.029762098249358826, 学習精度: 0.98055, テスト精度: 0.9696\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリをパスに追加\n",
    "from deep_learning_from_scratch.dataset.mnist import load_mnist  # MNISTデータセットを読み込む関数\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# MNISTデータセットを読み込む\n",
    "(img_train, label_train), (img_test, label_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)  # ネットワークを初期化\n",
    "\n",
    "# 学習の設定\n",
    "iters_num = 10000  # 学習の繰り返し回数\n",
    "train_size = img_train.shape[0]  # 学習データのサイズ\n",
    "batch_size = 100  # バッチサイズ\n",
    "learning_rate = 0.1  # 学習率\n",
    "\n",
    "# 学習のための変数を初期化\n",
    "train_loss_list = []  # 損失値を保存するリスト\n",
    "train_acc_list = []  # 学習データの精度を保存するリスト\n",
    "test_acc_list = []  # テストデータの精度を保存するリスト\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)  # 1エポックあたりのイテレーション数\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # バッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)  # ランダムにバッチサイズ分のインデックスを取得\n",
    "    x_batch = img_train[batch_mask]  # バッチの入力データ\n",
    "    t_batch = label_train[batch_mask]  # バッチの正解ラベル\n",
    "\n",
    "    # 勾配を計算\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # パラメータの更新\n",
    "    for key in network.params.keys():\n",
    "        network.params[key] -= learning_rate * grads[key]\n",
    "\n",
    "    # 損失値を計算\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 精度を計算\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(img_train, label_train)  # 学習データの精度\n",
    "        test_acc = network.accuracy(img_test, label_test)  # テストデータの精度\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"イテレーション {i}, 損失: {loss}, 学習精度: {train_acc}, テスト精度: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260a1ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
